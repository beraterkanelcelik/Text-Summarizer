# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11egEE4TEsWgtOpbwqPkyo9NE46VTCkkG
"""

!pip install datasets

!pip install evaluate

!pip install rouge_score

# train.py

import os
import torch
import matplotlib.pyplot as plt
import numpy as np

from datasets import load_dataset
import evaluate
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments

# Check for GPU (e.g., T4 GPU in Colab)
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)
if device == "cuda":
    print("GPU Name:", torch.cuda.get_device_name(0))

# Set model name and load tokenizer and model
model_name = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)
model.to(device)  # Move model to GPU if available

# Load the CNN/DailyMail dataset (version 3.0.0)
dataset = load_dataset("cnn_dailymail", "3.0.0")

# Use a larger subset for fine-tuning (for better results)
# Adjust these numbers based on your available resources.
train_dataset = dataset["train"].select(range(10000))
val_dataset = dataset["validation"].select(range(1000))

def preprocess_function(examples):
    # Add task prefix required by T5
    inputs = ["summarize: " + doc for doc in examples["article"]]
    # Tokenize inputs (articles) with padding and truncation
    model_inputs = tokenizer(
        inputs,
        max_length=512,
        truncation=True,
        padding="max_length"
    )
    # Tokenize targets (summaries/highlights) with padding and truncation
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples["highlights"],
            max_length=150,
            truncation=True,
            padding="max_length"
        )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Preprocess the datasets
train_dataset = train_dataset.map(preprocess_function, batched=True)
val_dataset = val_dataset.map(preprocess_function, batched=True)

# Define training arguments (using more epochs for better results)
training_args = TrainingArguments(
    output_dir="./results",              # Directory for checkpoints/results
    evaluation_strategy="steps",         # Evaluate at regular intervals
    eval_steps=100,                     # Evaluation frequency (adjusted for larger dataset)
    learning_rate=3e-4,                  # Slightly increased learning rate for more epochs
    per_device_train_batch_size=32,       # Batch size per GPU for training
    per_device_eval_batch_size=4,        # Batch size per GPU for evaluation
    num_train_epochs=5,                  # Increased number of training epochs
    weight_decay=0.01,                   # Weight decay for regularization
    save_steps=100,                     # Save checkpoint every 1000 steps
    fp16=True,                           # Enable mixed precision training
    logging_steps=200,                   # Logging frequency
    push_to_hub=False,                   # Do not push model to Hugging Face Hub automatically
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model and tokenizer
model.save_pretrained("t5_summarizer")
tokenizer.save_pretrained("t5_summarizer")

# -------------------------------
# Evaluation & Comparison Section
# -------------------------------

# Load the baseline (non-fine-tuned) T5-small model for comparison.
baseline_model = T5ForConditionalGeneration.from_pretrained(model_name)
baseline_model.to(device)
baseline_model.eval()

# Set both models to evaluation mode.
model.eval()

# For comparison, evaluate on 200 examples from the validation set.
eval_samples = dataset["validation"].select(range(200))

def generate_summary(model, text):
    # Prepend task prefix and tokenize input
    input_ids = tokenizer.encode(
        "summarize: " + text,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(device)
    # Generate summary using beam search
    summary_ids = model.generate(input_ids, max_length=150, num_beams=2, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

references = []
fine_tuned_preds = []
baseline_preds = []

# Generate summaries for each sample using both models.
for sample in eval_samples:
    article = sample["article"]
    reference = sample["highlights"]
    references.append(reference)

    fine_tuned_summary = generate_summary(model, article)
    baseline_summary = generate_summary(baseline_model, article)

    fine_tuned_preds.append(fine_tuned_summary)
    baseline_preds.append(baseline_summary)

# Load the ROUGE metric from the evaluate library
rouge_metric = evaluate.load("rouge")

# Compute ROUGE scores for the fine-tuned model.
fine_tuned_results = rouge_metric.compute(predictions=fine_tuned_preds, references=references)
# Compute ROUGE scores for the baseline model.
baseline_results = rouge_metric.compute(predictions=baseline_preds, references=references)

# Multiply by 100 to get percentage values
fine_tuned_scores = {
    "ROUGE-1": fine_tuned_results["rouge1"] * 100,
    "ROUGE-2": fine_tuned_results["rouge2"] * 100,
    "ROUGE-L": fine_tuned_results["rougeL"] * 100,
}
baseline_scores = {
    "ROUGE-1": baseline_results["rouge1"] * 100,
    "ROUGE-2": baseline_results["rouge2"] * 100,
    "ROUGE-L": baseline_results["rougeL"] * 100,
}

print("Fine-tuned model ROUGE scores:", fine_tuned_scores)
print("Baseline model ROUGE scores:", baseline_scores)

# -------------------------------
# Plotting the Comparison Graph
# -------------------------------
metrics = list(fine_tuned_scores.keys())
fine_tuned_values = [fine_tuned_scores[m] for m in metrics]
baseline_values = [baseline_scores[m] for m in metrics]

x = np.arange(len(metrics))  # Label locations
width = 0.35  # Width of bars

fig, ax = plt.subplots(figsize=(8, 6))
rects1 = ax.bar(x - width/2, baseline_values, width, label="Baseline")
rects2 = ax.bar(x + width/2, fine_tuned_values, width, label="Fine-tuned")

ax.set_ylabel("ROUGE F1 Score (%)")
ax.set_title("Comparison of ROUGE Scores: Baseline vs. Fine-tuned")
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.legend()

def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f"{height:.1f}",
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha="center", va="bottom")

autolabel(rects1)
autolabel(rects2)

plt.tight_layout()
plt.savefig("rouge_comparison.png")
print("Comparison graph saved as 'rouge_comparison.png'.")

!pip install huggingface_hub

!huggingface-cli login

# First, save the model and tokenizer locally
model.save_pretrained("my-t5-summarizer-model")
tokenizer.save_pretrained("my-t5-summarizer-model")

# Then, push them to the Hugging Face Hub
model.push_to_hub("my-t5-summarizer-model")
tokenizer.push_to_hub("my-t5-summarizer-model")

